{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcc4ae-02a7-4cb6-bde2-1d277b00b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('LDC data.xlsx')\n",
    "# Convert raw data to dimensionless parameters\n",
    "data['W/H'] = data['W'] / data['H']\n",
    "data['U/U*'] = data['U'] / data['U*']\n",
    "data['epsilon/HU*'] = data['epsilon'] / (data['H'] * data['U'])\n",
    "X = data[['W/H', 'U/U*', '(C)Nitrate (µg/L)']]\n",
    "y = data['epsilon/HU*']\n",
    "\n",
    "# Split the data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Standardize the data (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Define physical constraints for HHPS\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Extract input features (to be used for physical constraints)\n",
    "    W_H = tf.convert_to_tensor(X_train[:, 0], dtype=tf.float32)\n",
    "    U_U_star = tf.convert_to_tensor(X_train[:, 1], dtype=tf.float32)\n",
    "    CNitrate = tf.convert_to_tensor(X_train[:, 2], dtype=tf.float32)\n",
    "    y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    # Physical Parameters\n",
    "    rho = 1000  # Density of water in kg/m³\n",
    "    v = 1e-6    # Kinematic viscosity in m²/s\n",
    "    # Apply automatic differentiation to compute derivatives\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(W_H)\n",
    "        tape.watch(U_U_star)\n",
    "        tape.watch(CNitrate)\n",
    "        # Compute first order gradients\n",
    "        dy_dt = tape.gradient(y_pred_tensor, W_H)\n",
    "        dy_dx = tape.gradient(y_pred_tensor, U_U_star)\n",
    "        dy_dC = tape.gradient(y_pred_tensor, CNitrate)\n",
    "        # Compute second order gradients\n",
    "        d2y_dx2 = tape.gradient(dy_dx, U_U_star)\n",
    "        d2y_dtdt = tape.gradient(dy_dt, W_H)\n",
    "    # Computing constraint terms\n",
    "    # Mass Continuity: e_MC = |∂Q/∂x + ∂Q/∂t|^2\n",
    "    mass_continuity = tf.reduce_sum(tf.square(dy_dx + dy_dt))\n",
    "    # Momentum Conservation: e_MOM = |∂u/∂t + u ∂u/∂x - ν ∂²u/∂x²|^2\n",
    "    momentum_conservation = tf.reduce_sum(tf.square(dy_dC + U_U_star * dy_dx - v * d2y_dx2))\n",
    "    # Navier-Stokes: e_NS = |∂C/∂t + u ∂C/∂x - ε ∂²C/∂x²|^2\n",
    "    navier_stokes = tf.reduce_sum(tf.square(dy_dt + U_U_star * dy_dC - epsilon * d2y_dx2))\n",
    "    # Mean Squared Error Loss\n",
    "    mse = tf.keras.backend.mean(tf.keras.backend.square(y_true - y_pred))\n",
    "    # Combined Loss Function\n",
    "    total_loss = mse + 0.1 * (mass_continuity + momentum_conservation + navier_stokes)\n",
    "    return total_loss\n",
    "# HHPS Model (neural network)\n",
    "hhps_model = Sequential()\n",
    "hhps_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "hhps_model.add(Dropout(0.5))\n",
    "hhps_model.add(Dense(128, activation='relu'))\n",
    "hhps_model.add(Dropout(0.5))\n",
    "hhps_model.add(Dense(128, activation='relu'))\n",
    "hhps_model.add(Dropout(0.5))\n",
    "hhps_model.add(Dense(128, activation='relu'))\n",
    "hhps_model.add(Dropout(0.5))\n",
    "hhps_model.add(Dense(1))\n",
    "# Compile the HHPS model with custom loss function including the physical constraints\n",
    "hhps_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=custom_loss)\n",
    "# Train the HHPS model\n",
    "hhps_model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n",
    "# Predict using the HHPS model\n",
    "hhps_predictions = hhps_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cf081-bba5-4faa-a306-96d654c627fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('LDC data.xlsx')\n",
    "# Convert raw data to dimensionless parameters\n",
    "data['W/H'] = data['W'] / data['H']\n",
    "data['U/U*'] = data['U'] / data['U*']\n",
    "data['Epsilon/HU*'] = data['epsilon'] / (data['H'] * data['U'])\n",
    "data['C'] = data['(C)Nitrate (µg/L)']  # Ensure you have this column available\n",
    "X = data[['W/H', 'U/U*', 'C']]\n",
    "y = data['Epsilon/HU*']\n",
    "# Split the data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Standardize the data (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Define the physical constraints based on given equations\n",
    "def physics_constraints(y_true, y_pred, X):\n",
    "    W_H = X[:, 0]\n",
    "    U_U_star = X[:, 1]\n",
    "    concentration = X[:, 2]\n",
    "    rho = 1000  # Density of water in kg/m³\n",
    "    epsilon = 1e-3  # Example value for dispersion coefficient in m²/s (can be adjusted)\n",
    "    v = 1e-6  # Kinematic viscosity in m²/s\n",
    "    # Compute gradients using automatic differentiation\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(W_H)\n",
    "        tape.watch(U_U_star)\n",
    "        tape.watch(concentration)\n",
    "        dy_dt = tape.gradient(y_pred, W_H)\n",
    "        dy_dx = tape.gradient(y_pred, U_U_star)\n",
    "        dy_dc = tape.gradient(y_pred, concentration)\n",
    "        dy_d2x = tape.gradient(dy_dc, U_U_star)\n",
    "        dy_d2t = tape.gradient(dy_dt, W_H)\n",
    "    # Mass Continuity: |∂Q/∂x + ∂Q/∂t|^2\n",
    "    mass_continuity = K.sum(K.square(dy_dx + dy_dt)\n",
    "    # Momentum Conservation: |∂u/∂t + u ∂u/∂x - ν ∂²u/∂x²|^2\n",
    "    momentum_conservation = (\n",
    "        K.sum(K.square(dy_dc + U_U_star * dy_dx))  # ∂u/∂t + u ∂u/∂x\n",
    "        + K.sum(K.square(v * dy_d2x))  # v ∂²u/∂x²\n",
    "    )\n",
    "    # Navier-Stokes: |∂C/∂t + u ∂C/∂x - ε ∂²C/∂x²|^2\n",
    "    navier_stokes_term = (\n",
    "        K.sum(K.square(dy_dt))  # ∂C/∂t\n",
    "        + U_U_star * K.sum(K.square(dy_dx))  # u ∂C/∂x\n",
    "        - epsilon * K.sum(K.square(dy_d2x))  # ε ∂²C/∂x²\n",
    "    )\n",
    "    return K.sum(K.square(mass_continuity)) + K.sum(K.square(momentum_conservation)) + K.sum(K.square(navier_stokes_term))\n",
    "# Define a custom loss function that includes physical constraints\n",
    "def custom_loss(y_true, y_pred, X):\n",
    "    mse = K.mean(K.square(y_true - y_pred))\n",
    "    pc = physics_constraints(y_true, y_pred, X)\n",
    "    return mse + 0.3 * pc  # weighting factor for physical constraints\n",
    "def custom_loss_wrapper(X):\n",
    "    def loss(y_true, y_pred):\n",
    "        return custom_loss(y_true, y_pred, X)\n",
    "    return loss\n",
    "# PEML Model (Physics-Informed Neural Network)\n",
    "pinn_model = Sequential()\n",
    "pinn_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "pinn_model.add(Dropout(0.5))\n",
    "pinn_model.add(Dense(128, activation='relu'))\n",
    "pinn_model.add(Dropout(0.5))\n",
    "pinn_model.add(Dense(128, activation='relu'))\n",
    "pinn_model.add(Dropout(0.5))\n",
    "pinn_model.add(Dense(1))\n",
    "# Compile the PEML model with custom loss function including physical constraints\n",
    "pinn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   loss=custom_loss_wrapper(X_train))\n",
    "# Train the PEML model\n",
    "pinn_model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n",
    "# Predict using the PEML model\n",
    "pinn_predictions = pinn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe251167-f0c8-454b-8ca4-d834c83dac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('LDC data.xlsx')\n",
    "# Convert raw data to dimensionless parameters\n",
    "data['W/H'] = data['W'] / data['H']\n",
    "data['U/U*'] = data['U'] / data['U*']\n",
    "data['Epsilon/HU*'] = data['epsilon'] / (data['H'] * data['U'])\n",
    "data['C'] = data['(C)Nitrate (µg/L)']  # Ensure you have this column available\n",
    "X = data[['W/H', 'U/U*', 'C']]\n",
    "y = data['Epsilon/HU*']\n",
    "# Split the data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Standardize the data (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Define the physical constraints based on given equations\n",
    "def physics_constraints(y_true, y_pred, X):\n",
    "    W_H = X[:, 0]\n",
    "    U_U_star = X[:, 1]\n",
    "    concentration = X[:, 2]\n",
    "    rho = 1000  # Density of water in kg/m³\n",
    "    v = 1e-6  # Kinematic viscosity in m²/s\n",
    "    # Compute gradients using automatic differentiation\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(W_H)\n",
    "        tape.watch(U_U_star)\n",
    "        tape.watch(concentration)\n",
    "        y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "        dy_dt = tape.gradient(y_pred_tensor, W_H)\n",
    "        dy_dx = tape.gradient(y_pred_tensor, U_U_star)\n",
    "        dy_dc = tape.gradient(y_pred_tensor, concentration)\n",
    "        dy_d2x = tape.gradient(dy_dc, U_U_star)\n",
    "        dy_d2t = tape.gradient(dy_dt, W_H)\n",
    "    # Mass Continuity: |∂Q/∂x + ∂Q/∂t|^2\n",
    "    mass_continuity = K.sum(K.square(dy_dx + dy_dt))\n",
    "    # Momentum Conservation: |∂u/∂t + u ∂u/∂x - ν ∂²u/∂x²|^2\n",
    "    momentum_conservation = (\n",
    "        K.sum(K.square(dy_dc + U_U_star * dy_dx))  # ∂u/∂t + u ∂u/∂x\n",
    "        + K.sum(K.square(v * dy_d2x))  # v ∂²u/∂x²\n",
    "    )\n",
    "    # Navier-Stokes: |∂C/∂t + u ∂C/∂x - ε ∂²C/∂x²|^2\n",
    "    navier_stokes_term = (\n",
    "        K.sum(K.square(dy_dt))  # ∂C/∂t\n",
    "        + U_U_star * K.sum(K.square(dy_dx))  # u ∂C/∂x\n",
    "        - epsilon * K.sum(K.square(dy_d2x))  # ε ∂²C/∂x²\n",
    "    )\n",
    "    return K.sum(K.square(mass_continuity)) + K.sum(K.square(momentum_conservation)) + K.sum(K.square(navier_stokes_term))\n",
    "# Custom Loss Function for PRRT model\n",
    "class PhysicsRegularizedLoss(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_estimator, physics_regularization_weight=0.3):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.physics_regularization_weight = physics_regularization_weight\n",
    "    def fit(self, X, y):\n",
    "        self.base_estimator.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.base_estimator.predict(X)\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        pc = physics_constraints(y, y_pred, X)\n",
    "        total_loss = mse + self.physics_regularization_weight * pc\n",
    "        return -total_loss\n",
    "# Base Decision Tree Regressor model\n",
    "dtr = DecisionTreeRegressor(max_depth=5, \n",
    "                            min_samples_split=10)\n",
    "# PRRT model with custom loss\n",
    "prrt_model = PhysicsRegularizedLoss(base_estimator=dtr, \n",
    "                                    physics_regularization_weight=0.3)\n",
    "# Train the PRRT model\n",
    "prrt_model.fit(X_train, y_train)\n",
    "# Predict using the PRRT model\n",
    "prrt_predictions = prrt_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
